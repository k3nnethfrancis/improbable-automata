{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Newsletter LM: Improbable Automata\n",
    "Using Mirascope to build an LLM that can write a newsletter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "from mirascope.core import openai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The capital of Japan is Tokyo.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "def get_capital(country: str) -> str:\n",
    "    return f\"What is the capital of {country}?\"\n",
    "\n",
    "\n",
    "response = get_capital(\"Japan\")\n",
    "print(response.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## o1 thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(User): how many s's in the word mississssippi\n",
      "(Assistant): The word \"mississippi\" contains 5 's' characters.\n"
     ]
    }
   ],
   "source": [
    "# without chain of thought\n",
    "\n",
    "history: list[dict[str, str]] = []\n",
    "\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "def generate_answer(question: str) -> str:\n",
    "    return f\"Generate an answer to this question: {question}\"\n",
    "\n",
    "\n",
    "def run() -> None:\n",
    "    question: str = \"how many s's in the word mississssippi\"\n",
    "    response: str = generate_answer(question)\n",
    "    print(f\"(User): {question}\")\n",
    "    print(f\"(Assistant): {response}\")\n",
    "    history.append({\"role\": \"user\", \"content\": question})\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(User): How many s's are in the word 'mississssippi'?\n",
      "Step 1: Counting the s's in 'mississssippi':\n",
      "To determine how many 's's are present in the word 'mississssippi', I will start by breaking down the word. The word 'mississssippi' can be visually scanned for the letter 's'. I will count each 's' as I encounter it. The word structure indicates that there are groups of 's's: one 's' after the first 'm', two 's's following the first 'i', then an additional three 's's leading up to 'ippi'. As I tally, I find that the letters are indeed: 1 (from 'miss') + 2 (from 'iss') + 3 (from 'sss') = 6 s's in the total. I will verify if there can be an alternative method, such as writing each character and counting, but the visual and systematic scanning seems straightforward. Therefore, I conclude there are 6 's's. I need to decide if there should be another step to confirm or consider alternative aspects, or if I can finalize my answer now.\n",
      "**Thinking time: 4.06 seconds**\n",
      "\n",
      "Final Answer:\n",
      "6\n",
      "**Thinking time: 0.55 seconds**\n",
      "\n",
      "**Total thinking time: 4.61 seconds**\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "# from mirascope.core import groq\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "history: list[dict] = []\n",
    "\n",
    "\n",
    "class COTResult(BaseModel):\n",
    "    title: str = Field(..., desecription=\"The title of the step\")\n",
    "    content: str = Field(..., description=\"The output content of the step\")\n",
    "    next_action: Literal[\"continue\", \"final_answer\"] = Field(\n",
    "        ..., description=\"The next action to take\"\n",
    "    )\n",
    "\n",
    "\n",
    "# @groq.call(\"llama-3.1-70b-versatile\", json_mode=True, response_model=COTResult)\n",
    "@openai.call(\"gpt-4o-mini\", json_mode=True, response_model=COTResult)\n",
    "def cot_step(prompt: str, step_number: int, previous_steps: str) -> str:\n",
    "    return f\"\"\"\n",
    "    You are an expert AI assistant that explains your reasoning step by step.\n",
    "    For this step, provide a title that describes what you're doing, along with the content.\n",
    "    Decide if you need another step or if you're ready to give the final answer.\n",
    "\n",
    "    Guidelines:\n",
    "    - Use AT MOST 5 steps to derive the answer.\n",
    "    - Be aware of your limitations as an LLM and what you can and cannot do.\n",
    "    - In your reasoning, include exploration of alternative answers.\n",
    "    - Consider you may be wrong, and if you are wrong in your reasoning, where it would be.\n",
    "    - Fully test all other possibilities.\n",
    "    - YOU ARE ALLOWED TO BE WRONG. When you say you are re-examining\n",
    "        - Actually re-examine, and use another approach to do so.\n",
    "        - Do not just say you are re-examining.\n",
    "\n",
    "    IMPORTANT: Do not use code blocks or programming examples in your reasoning. Explain your process in plain language.\n",
    "\n",
    "    This is step number {step_number}.\n",
    "\n",
    "    Question: {prompt}\n",
    "\n",
    "    Previous steps:\n",
    "    {previous_steps}\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# @groq.call(\"llama-3.1-70b-versatile\")\n",
    "@openai.call(\"gpt-4o-mini\")\n",
    "def final_answer(prompt: str, reasoning: str) -> str:\n",
    "    return f\"\"\"\n",
    "    Based on the following chain of reasoning, provide a final answer to the question.\n",
    "    Only provide the text response without any titles or preambles.\n",
    "    Retain any formatting as instructed by the original prompt, such as exact formatting for free response or multiple choice.\n",
    "\n",
    "    Question: {prompt}\n",
    "\n",
    "    Reasoning:\n",
    "    {reasoning}\n",
    "\n",
    "    Final Answer:\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "def generate_cot_response(\n",
    "    user_query: str,\n",
    ") -> tuple[list[tuple[str, str, float]], float]:\n",
    "    steps: list[tuple[str, str, float]] = []\n",
    "    total_thinking_time: float = 0.0\n",
    "    step_count: int = 1\n",
    "    reasoning: str = \"\"\n",
    "    previous_steps: str = \"\"\n",
    "\n",
    "    while True:\n",
    "        start_time: datetime = datetime.now()\n",
    "        cot_result = cot_step(user_query, step_count, previous_steps)\n",
    "        end_time: datetime = datetime.now()\n",
    "        thinking_time: float = (end_time - start_time).total_seconds()\n",
    "\n",
    "        steps.append(\n",
    "            (\n",
    "                f\"Step {step_count}: {cot_result.title}\",\n",
    "                cot_result.content,\n",
    "                thinking_time,\n",
    "            )\n",
    "        )\n",
    "        total_thinking_time += thinking_time\n",
    "\n",
    "        reasoning += f\"\\n{cot_result.content}\\n\"\n",
    "        previous_steps += f\"\\n{cot_result.content}\\n\"\n",
    "\n",
    "        if cot_result.next_action == \"final_answer\" or step_count >= 5:\n",
    "            break\n",
    "\n",
    "        step_count += 1\n",
    "\n",
    "    # Generate final answer\n",
    "    start_time = datetime.now()\n",
    "    final_result: str = final_answer(user_query, reasoning).content\n",
    "    end_time = datetime.now()\n",
    "    thinking_time = (end_time - start_time).total_seconds()\n",
    "    total_thinking_time += thinking_time\n",
    "\n",
    "    steps.append((\"Final Answer\", final_result, thinking_time))\n",
    "\n",
    "    return steps, total_thinking_time\n",
    "\n",
    "\n",
    "def display_cot_response(\n",
    "    steps: list[tuple[str, str, float]], total_thinking_time: float\n",
    ") -> None:\n",
    "    for title, content, thinking_time in steps:\n",
    "        print(f\"{title}:\")\n",
    "        print(content.strip())\n",
    "        print(f\"**Thinking time: {thinking_time:.2f} seconds**\\n\")\n",
    "\n",
    "    print(f\"**Total thinking time: {total_thinking_time:.2f} seconds**\")\n",
    "\n",
    "\n",
    "def run() -> None:\n",
    "    question: str = \"How many s's are in the word 'mississssippi'?\"\n",
    "    print(\"(User):\", question)\n",
    "    # Generate COT response\n",
    "    steps, total_thinking_time = generate_cot_response(question)\n",
    "    display_cot_response(steps, total_thinking_time)\n",
    "\n",
    "    # Add the interaction to the history\n",
    "    history.append({\"role\": \"user\", \"content\": question})\n",
    "    history.append(\n",
    "        {\"role\": \"assistant\", \"content\": steps[-1][1]}\n",
    "    )  # Add only the final answer to the history\n",
    "\n",
    "\n",
    "# Run the function\n",
    "\n",
    "run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blog Writing Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the `BaseAgent`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import abstractmethod\n",
    "\n",
    "from mirascope.core import BaseMessageParam, openai\n",
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class OpenAIAgent(BaseModel):\n",
    "    history: list[BaseMessageParam | openai.OpenAIMessageParam] = []\n",
    "\n",
    "    @abstractmethod\n",
    "    def _step(self, prompt: str) -> openai.OpenAIStream: ...\n",
    "\n",
    "    def run(self, prompt: str) -> str:\n",
    "        stream = self._step(prompt)\n",
    "        result, tools_and_outputs = \"\", []\n",
    "\n",
    "        for chunk, tool in stream:\n",
    "            if tool:\n",
    "                tools_and_outputs.append((tool, tool.call()))\n",
    "            else:\n",
    "                result += chunk.content\n",
    "                print(chunk.content, end=\"\", flush=True)\n",
    "        if stream.user_message_param:\n",
    "            self.history.append(stream.user_message_param)\n",
    "        self.history.append(stream.message_param)\n",
    "        if tools_and_outputs:\n",
    "            self.history += stream.tool_message_params(tools_and_outputs)\n",
    "            return self.run(\"\")\n",
    "        print(\"\\n\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the `_step` function is marked as an abstract method that each subclass will need to implement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect\n",
    "\n",
    "from duckduckgo_search import DDGS\n",
    "\n",
    "\n",
    "class ResearcherBase(OpenAIAgent):\n",
    "    max_results: int = 10\n",
    "\n",
    "    def web_search(self, text: str) -> str:\n",
    "        \"\"\"Search the web for the given text.\n",
    "\n",
    "        Args:\n",
    "            text: The text to search for.\n",
    "\n",
    "        Returns:\n",
    "            The search results for the given text formatted as newline separated\n",
    "            dictionaries with keys 'title', 'href', and 'body'.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            results = DDGS(proxy=None).text(text, max_results=self.max_results)\n",
    "            return \"\\n\\n\".join(\n",
    "                [\n",
    "                    inspect.cleandoc(\n",
    "                        \"\"\"\n",
    "                        title: {title}\n",
    "                        href: {href}\n",
    "                        body: {body}\n",
    "                        \"\"\"\n",
    "                    ).format(**result)\n",
    "                    for result in results\n",
    "                ]\n",
    "            )\n",
    "        except Exception as e:\n",
    "            return f\"{type(e)}: Failed to search the web for text\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "class ResearcherBaseWithParser(ResearcherBase):\n",
    "    def parse_webpage(self, link: str) -> str:\n",
    "        \"\"\"Parse the paragraphs of the webpage found at `link`.\n",
    "\n",
    "        Args:\n",
    "            link: The URL of the webpage.\n",
    "\n",
    "        Returns:\n",
    "            The parsed paragraphs of the webpage, separated by newlines.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(link)\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            return \"\\n\".join([p.text for p in soup.find_all(\"p\")])\n",
    "        except Exception as e:\n",
    "            return f\"{type(e)}: Failed to parse content from URL\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Researcher Step Fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mirascope.core import prompt_template\n",
    "\n",
    "\n",
    "class ResearcherBaseWithStep(ResearcherBaseWithParser):\n",
    "    @openai.call(\"gpt-4o-mini\", stream=True)\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        SYSTEM:\n",
    "        Your task is to research a topic and summarize the information you find.\n",
    "        This information will be given to a writer (user) to create a blog post.\n",
    "\n",
    "        You have access to the following tools:\n",
    "        - `web_search`: Search the web for information. Limit to max {self.max_results}\n",
    "            results.\n",
    "        - `parse_webpage`: Parse the content of a webpage.\n",
    "\n",
    "        When calling the `web_search` tool, the `body` is simply the body of the search\n",
    "        result. You MUST then call the `parse_webpage` tool to get the actual content\n",
    "        of the webpage. It is up to you to determine which search results to parse.\n",
    "\n",
    "        Once you have gathered all of the information you need, generate a writeup that\n",
    "        strikes the right balance between brevity and completeness. The goal is to\n",
    "        provide as much information to the writer as possible without overwhelming them.\n",
    "\n",
    "        MESSAGES: {self.history}\n",
    "        USER: {prompt}\n",
    "        \"\"\"\n",
    "    )\n",
    "    def _step(self, prompt: str) -> openai.OpenAIDynamicConfig:\n",
    "        return {\"tools\": [self.web_search, self.parse_webpage]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing the `research` tool method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we could use the `run` method from our `OpenAIAgent` as a tool, there is value in further engineering our prompt by providing good descriptions (and names!) for the tools we use. Putting everything together, we can expose a `research` method that we can later use as a tool in our agent executor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Researcher(ResearcherBaseWithStep):\n",
    "    def research(self, prompt: str) -> str:\n",
    "        \"\"\"Research a topic and summarize the information found.\n",
    "\n",
    "        Args:\n",
    "            prompt: The user prompt to guide the research. The content of this prompt\n",
    "                is directly responsible for the quality of the research, so it is\n",
    "                crucial that the prompt be clear and concise.\n",
    "\n",
    "        Returns:\n",
    "            The results of the research.\n",
    "        \"\"\"\n",
    "        print(\"RESEARCHING...\")\n",
    "        result = self.run(prompt)\n",
    "        print(\"RESEARCH COMPLETE!\")\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mirascope.integrations.tenacity import collect_errors\n",
    "from pydantic import ValidationError\n",
    "from tenacity import retry, wait_exponential\n",
    "\n",
    "\n",
    "class AgentExecutorBase(OpenAIAgent):\n",
    "    researcher: Researcher = Researcher()\n",
    "    num_paragraphs: int = 4\n",
    "\n",
    "    class InitialDraft(BaseModel):\n",
    "        draft: str\n",
    "        critique: str\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_initial_draft(response: InitialDraft) -> str:\n",
    "        return f\"Draft: {response.draft}\\nCritique: {response.critique}\"\n",
    "\n",
    "    @retry(\n",
    "        wait=wait_exponential(multiplier=1, min=4, max=10),\n",
    "        after=collect_errors(ValidationError),\n",
    "    )\n",
    "    @openai.call(\n",
    "        \"gpt-4o-mini\", response_model=InitialDraft, output_parser=parse_initial_draft\n",
    "    )\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        SYSTEM:\n",
    "        Your task is to write the initial draft for a blog post based on the information\n",
    "        provided to you by the researcher, which will be a summary of the information\n",
    "        they found on the internet.\n",
    "\n",
    "        Along with the draft, you will also write a critique of your own work. This\n",
    "        critique is crucial for improving the quality of the draft in subsequent\n",
    "        iterations. Ensure that the critique is thoughtful, constructive, and specific.\n",
    "        It should strike the right balance between comprehensive and concise feedback.\n",
    "\n",
    "        If for any reason you deem that the research is insufficient or unclear, you can\n",
    "        request that additional research be conducted by the researcher. Make sure that\n",
    "        your request is specific, clear, and concise.\n",
    "\n",
    "        MESSAGES: {self.history}\n",
    "        USER:\n",
    "        {previous_errors}\n",
    "        {prompt}\n",
    "        \"\"\"\n",
    "    )\n",
    "    def _write_initial_draft(\n",
    "        self, prompt: str, *, errors: list[ValidationError] | None = None\n",
    "    ) -> openai.OpenAIDynamicConfig:\n",
    "        \"\"\"Writes the initial draft of a blog post along with a self-critique.\n",
    "\n",
    "        Args:\n",
    "            prompt: The user prompt to guide the writing process. The content of this\n",
    "                prompt is directly responsible for the quality of the blog post, so it\n",
    "                is crucial that the prompt be clear and concise.\n",
    "\n",
    "        Returns:\n",
    "            The initial draft of the blog post along with a self-critique.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"computed_fields\": {\n",
    "                \"previous_errors\": f\"Previous Errors: {errors}\" if errors else None\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few things worth noting here:\n",
    "\n",
    "- We are again using `self` for convenient access to the containing class' state. In this case we expect to put this function inside of our executor and want to give access to the conversation history -- particularly the results of the researcher.\n",
    "- We are using `response_model` to extract specifically the `draft` and `critique` fields.\n",
    "- We are using an output parser `parse_initial_draft` to parse the `InitialDraft` class into a format that is friendly for using tools (str).\n",
    "- We are using tenacity in order to retry should the call fail to properly generate an `InitialDraft` instance, reinserting the list of previous errors into each subsequent call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Executor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we just need to put it all together into our AgentExecutor class, write our _step function, and run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING AGENT EXECUTION...\n",
      "RESEARCHING...\n",
      "### Large Language Models (LLMs): Overview\n",
      "\n",
      "Large Language Models (LLMs) are sophisticated AI systems trained on vast datasets to understand and generate human language. Built upon deep learning architectures, particularly the transformer model, they excel at various tasks including writing, summarization, translation, and conversation. Notable examples include OpenAI's GPT series, Google's BERT, and Meta's LLaMA, all leveraging billions of parameters to discern nuanced patterns in language. As a result, LLMs can create coherent, contextually relevant text and engage in human-like conversations, making them integral to advancements in natural language processing (NLP) and artificial intelligence (AI) applications.\n",
      "\n",
      "### Relationship to Structured Outputs\n",
      "\n",
      "LLMs can generate structured outputs, such as tables, formatted documents, or specific data queries (like SQL), based on unstructured inputs. Through techniques like prompt engineering and retrieval-augmented generation (RAG), LLMs can be fine-tuned to produce structured data responses that are contextually accurate and relevant to users' queries. This capability is essential for applications needing precise data extraction or representation, thereby enhancing the usability of AI in business contexts.\n",
      "\n",
      "### Benefits of LLMs for Structured Outputs\n",
      "\n",
      "1. **Enhanced Automation**: LLMs can automate data generation processes, reducing the need for manual entry and speeding up workflows.\n",
      "2. **Personalization**: They can analyze user data and interaction history to tailor outputs according to user preferences or business requirements.\n",
      "3. **Scalability**: Capable of handling vast amounts of data efficiently, LLMs allow organizations to scale their operations without proportionately increasing effort or resources.\n",
      "4. **Natural Language Interaction**: Users can interact with systems using natural language, making tech more approachable and less intimidating.\n",
      "\n",
      "### Challenges of LLMs\n",
      "\n",
      "1. **Bias**: LLMs can inherit biases present in their training datasets, leading to skewed or problematic outputs.\n",
      "2. **Environmental Impact**: The computational resources required for training large models are substantial, raising concerns about their carbon footprint.\n",
      "3. **Interpretability**: Understanding how LLMs arrive at specific outputs can be complex, complicating trust and accountability in AI processes.\n",
      "4. **Data Privacy**: As LLMs learn from large datasets, ensuring that sensitive information is protected remains a critical issue.\n",
      "\n",
      "### Applications of LLMs in Generating Structured Outputs\n",
      "\n",
      "- **Customer Service**: LLMs are utilized to power chatbots that can categorize and structure responses based on user queries, thereby improving customer interactions.\n",
      "- **Content Creation**: Automated summarization and report generation transform unstructured text into formatted outputs, valuable for both business reporting and academic research.\n",
      "- **Healthcare**: Extracting structured data from medical records enables healthcare professionals to make informed decisions quickly.\n",
      "- **Financial Analysis**: LLMs help in generating queries to analyze trends in financial datasets, providing structured insights that assist in risk management.\n",
      "- **Code Generation**: They can convert natural language commands into structured programming code, enhancing development processes.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "Large Language Models represent a transformative shift in how organizations utilize AI for generating structured outputs. Their ability to integrate unstructured input and produce coherent, tailored responses enhances productivity across various sectors. However, ongoing efforts must address associated challenges to fully leverage their potential while mitigating risks. As LLMs evolve, they are likely to become even more integral to business intelligence and operational efficiency.\n",
      "\n",
      "RESEARCH COMPLETE!\n",
      "The latest draft is a well-structured and engaging discussion on Large Language Models (LLMs) and their capabilities regarding structured outputs. It successfully integrates real-world examples and addresses the associated challenges while remaining concise and informative. Here is the final draft based on the improvements made through previous iterations:\n",
      "\n",
      "---\n",
      "\n",
      "### Understanding Large Language Models and Their Structured Output Capabilities\n",
      "\n",
      "Large Language Models (LLMs) represent a significant breakthrough in artificial intelligence, designed to interpret and generate human language through advanced deep learning techniques. These models, such as OpenAI's GPT and Google's BERT, are trained on extensive datasets, allowing them to recognize intricate patterns and contextual relationships in language. While these powerful tools excel at producing coherent and relevant textual responses, their adaptability extends beyond mere conversation—they can also generate structured outputs that are essential for various applications.\n",
      "\n",
      "The relationship between LLMs and structured outputs lies in their unique ability to convert unstructured inputs into organized data formats. By refining prompt engineering and employing methods like retraining or fine-tuning, LLMs can produce outputs such as tables, spreadsheets, or formatted text documents. Their capability to seamlessly transform qualitative information into a structured form enhances their usability across multiple domains, providing organizations with the data they require in a more digestible format.\n",
      "\n",
      "Employing LLMs for generating structured outputs offers numerous benefits, including automation that reduces manual effort in data entry and increases efficiency in business processes. Organizations implementing LLMs have seen an average of 25% improvement in operational efficiency through automated workflows. However, challenges persist, notably biases embedded in training datasets and data privacy concerns. For instance, IBM faced scrutiny when their Watson AI was criticized for biased recommendations in healthcare applications. After implementing fairness audits and revising their training data processes, they reported a 30% reduction in biased outputs. Additionally, OpenAI's earlier models inadvertently exposed user data, leading them to adopt strict governance protocols that have decreased privacy-related incidents by 40%.\n",
      "\n",
      "Future advancements in LLM technologies will focus heavily on enhancing ethical guidelines as organizations prioritize AI governance frameworks. These frameworks typically include components like bias impact assessments, data privacy compliance checks, and transparency protocols that document AI decision-making processes. By 2026, experts predict that 65% of organizations will adopt such frameworks, reflecting a concerted effort to tackle biases and enhance user data protection. Addressing these challenges thoughtfully will empower LLMs to reshape productivity while navigating the complexities of the digital age effectively.\n",
      "\n",
      "---\n",
      "\n",
      "This draft presents a thorough overview of LLMs, their structured output capabilities, and the associated challenges organizations face. It combines practical examples with predictions for the future, giving readers a comprehensive view of the current landscape and emerging trends. If you have any further adjustments or additions, please let me know!\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The latest draft is a well-structured and engaging discussion on Large Language Models (LLMs) and their capabilities regarding structured outputs. It successfully integrates real-world examples and addresses the associated challenges while remaining concise and informative. Here is the final draft based on the improvements made through previous iterations:\\n\\n---\\n\\n### Understanding Large Language Models and Their Structured Output Capabilities\\n\\nLarge Language Models (LLMs) represent a significant breakthrough in artificial intelligence, designed to interpret and generate human language through advanced deep learning techniques. These models, such as OpenAI's GPT and Google's BERT, are trained on extensive datasets, allowing them to recognize intricate patterns and contextual relationships in language. While these powerful tools excel at producing coherent and relevant textual responses, their adaptability extends beyond mere conversation—they can also generate structured outputs that are essential for various applications.\\n\\nThe relationship between LLMs and structured outputs lies in their unique ability to convert unstructured inputs into organized data formats. By refining prompt engineering and employing methods like retraining or fine-tuning, LLMs can produce outputs such as tables, spreadsheets, or formatted text documents. Their capability to seamlessly transform qualitative information into a structured form enhances their usability across multiple domains, providing organizations with the data they require in a more digestible format.\\n\\nEmploying LLMs for generating structured outputs offers numerous benefits, including automation that reduces manual effort in data entry and increases efficiency in business processes. Organizations implementing LLMs have seen an average of 25% improvement in operational efficiency through automated workflows. However, challenges persist, notably biases embedded in training datasets and data privacy concerns. For instance, IBM faced scrutiny when their Watson AI was criticized for biased recommendations in healthcare applications. After implementing fairness audits and revising their training data processes, they reported a 30% reduction in biased outputs. Additionally, OpenAI's earlier models inadvertently exposed user data, leading them to adopt strict governance protocols that have decreased privacy-related incidents by 40%.\\n\\nFuture advancements in LLM technologies will focus heavily on enhancing ethical guidelines as organizations prioritize AI governance frameworks. These frameworks typically include components like bias impact assessments, data privacy compliance checks, and transparency protocols that document AI decision-making processes. By 2026, experts predict that 65% of organizations will adopt such frameworks, reflecting a concerted effort to tackle biases and enhance user data protection. Addressing these challenges thoughtfully will empower LLMs to reshape productivity while navigating the complexities of the digital age effectively.\\n\\n---\\n\\nThis draft presents a thorough overview of LLMs, their structured output capabilities, and the associated challenges organizations face. It combines practical examples with predictions for the future, giving readers a comprehensive view of the current landscape and emerging trends. If you have any further adjustments or additions, please let me know!\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class AgentExecutor(AgentExecutorBase):\n",
    "    @openai.call(\"gpt-4o-mini\", stream=True)\n",
    "    @prompt_template(\n",
    "        \"\"\"\n",
    "        SYSTEM:\n",
    "        Your task is to facilitate the collaboration between the researcher and the\n",
    "        blog writer. The researcher will provide the blog writer with the information\n",
    "        they need to write a blog post, and the blog writer will draft and critique the\n",
    "        blog post until they reach a final iteration they are satisfied with.\n",
    "\n",
    "        To access the researcher and writer, you have the following tools:\n",
    "        - `research`: Prompt the researcher to perform research.\n",
    "        - `_write_initial_draft`: Write an initial draft with a self-critique\n",
    "\n",
    "        You will need to manage the flow of information between the researcher and the\n",
    "        blog writer, ensuring that the information provided is clear, concise, and\n",
    "        relevant to the task at hand.\n",
    "\n",
    "        The final blog post MUST have EXACTLY {self.num_paragraphs} paragraphs.\n",
    "\n",
    "        MESSAGES: {self.history}\n",
    "        USER: {prompt}\n",
    "        \"\"\"\n",
    "    )\n",
    "    def _step(self, prompt: str) -> openai.OpenAIDynamicConfig:\n",
    "        # Create function wrappers that don't include validation error handling\n",
    "        def research(prompt: str) -> str:\n",
    "            return self.researcher.research(prompt)\n",
    "            \n",
    "        def write_initial_draft(prompt: str) -> str:\n",
    "            return self._write_initial_draft(prompt)\n",
    "            \n",
    "        return {\"tools\": [research, write_initial_draft]}\n",
    "\n",
    "\n",
    "agent = AgentExecutor()\n",
    "print(\"STARTING AGENT EXECUTION...\")\n",
    "agent.run(\"Help me write a blog post about LLMs and structured outputs.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
